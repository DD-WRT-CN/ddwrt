Index: smp.c
===================================================================
--- smp.c	(revision 17154)
+++ smp.c	(working copy)
@@ -39,6 +39,7 @@
 #include <asm/tlbflush.h>
 #include <asm/ptrace.h>
 #include <asm/localtimer.h>
+#include <linux/dma-mapping.h>
 
 /*
  * as from 2.5, kernels no longer have an init_tasks structure
@@ -47,12 +48,20 @@
  */
 struct secondary_data secondary_data;
 
+#ifdef CONFIG_CPU_NO_CACHE_BCAST_DEBUG
+static DEFINE_PER_CPU(unsigned long,dma_cache_counter) = 0;
+#endif
+unsigned long bcache_bitmap = 0;
+
 enum ipi_msg_type {
 	IPI_TIMER = 2,
 	IPI_RESCHEDULE,
 	IPI_CALL_FUNC,
 	IPI_CALL_FUNC_SINGLE,
 	IPI_CPU_STOP,
+#ifdef CONFIG_CPU_NO_CACHE_BCAST
+	IPI_DMA_CACHE,
+#endif
 };
 
 int __cpuinit __cpu_up(unsigned int cpu)
@@ -376,6 +385,15 @@
 	}
 }
 
+static void send_ipi_message_cache(const struct cpumask *mask)
+{
+	unsigned long flags;
+
+	local_irq_save(flags);
+	smp_cross_call_cache(mask);
+	local_irq_restore(flags);
+}
+
 void arch_send_call_function_ipi_mask(const struct cpumask *mask)
 {
 	smp_cross_call(mask, IPI_CALL_FUNC);
@@ -407,6 +425,12 @@
 				   __get_irq_stat(cpu, ipi_irqs[i]));
 
 		seq_printf(p, " %s\n", ipi_types[i]);
+#ifdef CONFIG_CPU_NO_CACHE_BCAST_DEBUG
+	seq_puts(p, " dc: ");
+	for_each_present_cpu(cpu)
+		seq_printf(p, " %10lu", per_cpu(dma_cache_counter, cpu));
+	seq_putc(p, '\n');
+#endif
 	}
 }
 
@@ -543,6 +567,10 @@
 		cpu_relax();
 }
 
+#ifdef CONFIG_CPU_NO_CACHE_BCAST
+static void ipi_dma_cache_op(unsigned int cpu);
+#endif
+
 /*
  * Main handler for inter-processor interrupts
  */
@@ -578,6 +606,16 @@
 		ipi_cpu_stop(cpu);
 		break;
 
+#ifdef CONFIG_CPU_NO_CACHE_BCAST
+			case IPI_DMA_CACHE:
+#ifdef CONFIG_CPU_NO_CACHE_BCAST_DEBUG
+				//get_cpu_var(dma_cache_counter)++;
+				//put_cpu_var(dma_cache_counter);
+#endif
+				ipi_dma_cache_op(cpu);
+				break;
+#endif
+
 	default:
 		printk(KERN_CRIT "CPU%u: Unknown IPI message 0x%x\n",
 		       cpu, ipinr);
@@ -586,6 +624,18 @@
 	set_irq_regs(old_regs);
 }
 
+#ifdef CONFIG_CPU_NO_CACHE_BCAST
+asmlinkage void __exception do_cache_IPI(struct pt_regs *regs)
+{
+	unsigned int cpu = smp_processor_id();
+	struct pt_regs *old_regs = set_irq_regs(regs);
+
+
+	ipi_dma_cache_op(cpu);
+
+	set_irq_regs(old_regs);
+}
+#endif
 void smp_send_reschedule(int cpu)
 {
 	smp_cross_call(cpumask_of(cpu), IPI_RESCHEDULE);
@@ -618,3 +668,115 @@
 {
 	return -EINVAL;
 }
+
+#ifdef CONFIG_CPU_NO_CACHE_BCAST
+/*
+ * DMA cache maintenance operations on SMP if the automatic hardware
+ * broadcasting is not available
+ */
+struct smp_dma_cache_struct {
+	int type;
+	const void *start;
+	const void *end;
+	char unfinished;
+};
+
+static struct smp_dma_cache_struct smp_dma_cache_data[3];
+static DEFINE_SPINLOCK(smp_dma_cache_lock);
+
+static void local_dma_cache_op(int type, const void *start, const void *end)
+{
+	switch (type) {
+	case SMP_DMA_CACHE_INV:
+		dmac_map_area(start, (size_t)(end-start),DMA_FROM_DEVICE);
+		break;
+	case SMP_DMA_CACHE_CLEAN:
+		dmac_unmap_area(start, (size_t)(end-start),DMA_FROM_DEVICE);
+		break;
+	case SMP_DMA_CACHE_FLUSH:
+		dmac_flush_range(start, end);
+		break;
+	default:
+		printk(KERN_CRIT "CPU%u: Unknown SMP DMA cache type %d\n",
+		       smp_processor_id(), type);
+	}
+}
+
+/*
+ * This function must be executed with interrupts disabled.
+ */
+static void ipi_dma_cache_op(unsigned int cpu)
+{
+	unsigned long flags;
+	int type;
+	const void *start;
+	const void *end;
+
+	/* check for spurious IPI */
+	spin_lock_irqsave(&smp_dma_cache_lock, flags);
+	if (!test_bit(cpu, &bcache_bitmap))
+		goto out;
+
+	type = smp_dma_cache_data[cpu].type;
+	start = smp_dma_cache_data[cpu].start;
+	end = smp_dma_cache_data[cpu].end;
+	spin_unlock_irqrestore(&smp_dma_cache_lock, flags);
+
+
+	local_dma_cache_op(type, start, end);
+
+	spin_lock_irqsave(&smp_dma_cache_lock, flags);
+	clear_bit(cpu, &bcache_bitmap);
+	smp_dma_cache_data[cpu].type = 0;
+	smp_dma_cache_data[cpu].start = 0;
+	smp_dma_cache_data[cpu].end = 0;
+	smp_dma_cache_data[cpu].unfinished = 0;
+out:
+	spin_unlock_irqrestore(&smp_dma_cache_lock, flags);
+}
+
+/*
+ * Execute the DMA cache operations on all online CPUs. This function
+ * can be called with interrupts disabled or from interrupt context.
+ */
+static void __smp_dma_cache_op(int type, const void *start, const void *end)
+{
+	cpumask_t callmap = cpu_online_map;
+	unsigned int cpu = get_cpu();
+	unsigned long flags;
+	unsigned long cpu_check;
+	cpu_clear(cpu, callmap);
+	cpu_check = *cpus_addr(callmap) >> 1;
+
+	while (test_bit(cpu, &bcache_bitmap))
+		ipi_dma_cache_op(cpu);
+
+	while (test_bit(cpu_check, &bcache_bitmap))
+		barrier();
+
+	spin_lock_irqsave(&smp_dma_cache_lock, flags);
+	smp_dma_cache_data[cpu_check].type = type;
+	smp_dma_cache_data[cpu_check].start = start;
+	smp_dma_cache_data[cpu_check].end = end;
+	smp_dma_cache_data[cpu_check].unfinished = 1;
+	set_bit(cpu_check, &bcache_bitmap);
+	send_ipi_message_cache(&callmap);
+	spin_unlock_irqrestore(&smp_dma_cache_lock, flags);
+
+	/* run the local operation in parallel with the other CPUs */
+	local_dma_cache_op(type, start, end);
+	put_cpu();
+}
+
+#define DMA_MAX_RANGE		SZ_4K
+
+/*
+ * Split the cache range in smaller pieces if interrupts are enabled
+ * to reduce the latency caused by disabling the interrupts during the
+ * broadcast.
+ */
+void smp_dma_cache_op(int type, const void *start, const void *end)
+{
+	__smp_dma_cache_op(type, start, end);
+}
+#endif
